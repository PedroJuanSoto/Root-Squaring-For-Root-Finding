% Fast computation of higher  order derivatives of a black box  function.


% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
% \newtheorem{remark}{Remark}

%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
%
\title{Fast computation of higher  order derivatives of a black box  function}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Pedro Soto\inst{1}\orcidID{0000-0002-7120-7362} \and
Soo Go\inst{2} \and
Victor Pan\inst{3}}
%
\authorrunning{Soto, Go, and Pan}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{The Graduate Center, CUNY
New York, NY, USA \email{psoto@gradcenter.cuny.edu}
\url{https://pedrojuansoto.github.io/}\\ \and
The Graduate Center, CUNY
New York, NY, USA \email{sgo@gradcenter.cuny.edu}\\ \and
Lehman College, CUNY
New York, NY, USA
\
\email{victor.pan@lehman.cuny.edu}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

\keywords{symbolic-numeric computing \and  root finding \and polynomial algorithms \and computer algebra.}
\end{abstract}
%
%
%
%changed simultaneously -> independently since there was a span of 10 years from Dandelin's discovery to Gr{\"a}ffe's

\section{Background and Motivation}\label{srrapp}
Let $p(x)$ be a polynomial with \emph{roots} $x_1, ..., x_d \in \mathbb{C}$ so that
\begin{equation}\label{eqpoly}
p(x) = \sum_{i=0}^{d} p_i x^i = p_d \prod_{i=1}^d (x-x_i), \; p_d \neq 0
\end{equation} 
%with $|x_d| > |x_{d-1}| > \cdots > |x_1|$.


The reverse polynomial $p_{\rm rev}$ of $p(x)$ is defined as 
 \begin{equation}\label{eqpolyrev}
p_{\rm rev}(x):=x^dp\Big(\frac{1}{x}\Big)=\sum_{i=0}^dp_ix^{d-i},~
p_{\rm rev}(x)=p_0\prod_{j=1}^d\Big(x-\frac{1}{x_j}\Big)~{\rm if}~p_0\neq 0.
\end{equation} 

%We describe some ways to gain insight into the roots $x_1, ..., x_d$ of $p(x)$, by computing the roots themselves and by establishing the bounds on the extremal root-radii. 

%\subsection{ Estimation of extremal root radii}
%[from the other paper]
\subsection{Root-squaring: DLG and FG root-finders}
DLG root-squaring iterations (cf. \cite{10.2307/2310626}) are defined by
\begin{equation}\label{eqdnd}
 p_0(x)=\frac{1}{p_d}p(x),~p_{k+1}(x^2)=(-1)^ d p_k(x)
p_k(-x),~k=0,1,\dots.\ell
\end{equation}
for a fixed positive integer $\ell$.

TODO: description of the DLG root-finder, including the following key expressions:

Let the  roots of equation $p(x)=0$ satisfy
\begin{equation}\label{eqnndcrs} 
 |x_1|\ge|x_2|\ge\cdots\ge |x_{d-i+1}|>\max_{j>d-i+1}|x_j|.
 \end{equation} 

We use the fact
 \begin{equation}\label{eqdndrt0}\frac{p_{k}'(0)}{p_{k}(0)}=\Big(\frac{p_{k-1}'(x)}{p_{k-1}(x)}\Big)'_{x=0}=\Big(\frac{p'(x)}{p(x)}\Big)^{(k)}_{x=0},~k=1,2,\dots
\end{equation}
Notice an immediate extension:
\begin{equation}\label{eqdndrth} \frac{p_{{\ell}}^{(k)}(0)}{p_{\ell}(0)}=\prod_{g=1}^k\Bigg(\frac{p^{(g)}(x)}{p^{(g-1)}(x)}\Bigg)_{x=0}^{(\ell)},~k=1,2,\dots.
\end{equation}

FG iterations fixes a polynomial $q(x)$ and computes
\begin{equation}\label{fg}
q_0(x)=q(x),~q_{i+1}(x^2)= ...
\end{equation}

TODO: description of the FG root-finder, including the following root radius assumption 

\begin{equation}\label{eqsprn}
|x_1|>|x_2|>\cdots>|x_d|.
\end{equation}
 

The companion paper [] presents the root-finders in more detail.

\subsection{DLG iterations for black box polynomials}
%challenge in computing them
Given the coefficients of
$p_i(x)$
we can reduce the $i$th root-squaring iteration, that is, the computation of
the coefficients of  $p_{i+1}(x)$, to polynomial multiplication and perform it in $O(d\log(d))$ arithmetic operations.
Unless the positive integer $\ell$ is small, the absolute values of the coefficients
of $p_{\ell}(x)$ vary  dramatically, and realistically one should either stop because of severe problems of numerical stability or apply the stable algorithm by Gregorio Malajovich and Jorge  P. Zubelli \cite{Malajovich2001OnTG}, which performs a single root-squaring at  arithmetic cost of order $d^2$.

%
For a black box polynomial $p(x)$, however, we apply DLG iterations without computing the coefficients, and the algorithm turns out to be quite efficient:
 for $\ell$ iterations  evaluate
 $p(x)$ at  $2^{\ell}$ equally spaced points on a circle and obtain
 the values of the polynomial $p_{\ell}(x)=\prod\left(x-x_j^{2^{\ell}}\right)$
 at these $2^{\ell}$ points.



\subsection{Estimation of extremal root radii}\label{sestextrrtrd}

%\subsection{FG iterations (roots)}
%(see the companion paper [] for more details and analysis)
%\\\\

If we do not know whether assumptions (\ref{eqnndcrs}) and (\ref{eqsprn}) hold, we can apply
 the following well-known bounds on the extremal root radii:

 \begin{equation}\label{eqrtrdbndsrev1} 
|x_d|\le d~\Big |\frac{p(0)}{p'(0)}\Big |~{\rm and}~|x_1|\ge \Big |\frac{p'_{\rm rev}(0)}{d~p_{\rm rev}(0)}\Big|.
\end{equation}

We can deduce these bounds 
from the well-known expression
\begin{equation}\label{eqratio}
\frac{p'(x)}{p(x)}=
\sum_{j=1}^d\frac{1}{x-x_j},
\end{equation} 
which we can obtain by differentiating the equation
$p(x)=p_d\prod_{j=1}^d(x-x_j)$.

By extending these bounds to
the polynomial $p_k(x)$ of Eq. (\ref{eqdnd}) we obtain that

 \begin{equation}\label{eqratio0} 
 |x_d|^{2^k}\le d/ 
\Big| \Big(\frac{p'(x)}{p(x)}\Big)_{x=0}^{(k)}\Big|~{\rm and}~|x_1|^{2^k}\ge \frac{1}{d}\Big| \Big(\frac{p'_{\rm rev}(x)}{p_{\rm rev}(x)}\Big)_{x=0}^{(k)}\Big|.
\end{equation}

Under the 
assumptions (\ref{eqnndcrs}) for $i=2$ for $p(x)$ and for $p_{\rm rev}(x)$, respectively,  the latter two bounds 
 become sharp as $k$ increases, by virtue of (\ref{eqrtrdbndsrev1}), and next we  argue informally that it tends to be sharp with a high probability under  random root models.
Indeed, 
\begin{equation}\label{eqratiorcp}
\frac{1}{|x_d|}\le \frac{1}{d}\Big|\frac{p'(c)}{p(c)}\Big|=\frac{1}{d}\Big|\sum_{j=1}^d \frac{1}{c-x_j}\Big|
\end{equation}  
by  virtue of 
(\ref{eqrtrdbndsrev1}), and so the
 approximation  
to the root radius $|x_d|$ is poor if and only if severe cancellation occurs in the summation of the $d$ roots, and similarly for the approximation
of  $r_1(c,p)$. Such a cancellation only occurs for a narrow class of polynomials $p(x)$, with a low  probability if we assume a random root model.
 
 Next we prove, however,  that estimates (\ref{eqrtrdbndsrev1}) and (\ref{eqratio0}) are extremely poor for  worst case inputs.
\begin{theorem}\label{thrtr}
The ratios $|\frac{p(0)}{p'(0)}|$ 
and $|\frac{p_{\rm rev}(0)}{p_{\rm rev}'(0)}|$ are infinite for  $p(x)=x^d-h^d$ and $h\neq 0$, while  $|x_d|)=|x_1|=|h|$.
 \end{theorem}
 \begin{proof}
% \begin{example}\label{exrtr} 
 Observe that 
 the roots $x_j=h\exp(\frac{(j-1){\bf i}}{2\pi d})$ of $p(x)=x^d-h^d$
  for $j=1,2,\dots,d$   are the $d$th roots  of unity up to scaling by $h$.
 \end{proof}
%Such poor approximations only occur for a %narrow input class.  

The problem persists for the root radius $r_d(w,p)$  where  $p'(w)$ and  $p'_{\rm rev}(w)$ vanish;  
 rotation of the variable  $p(x)\leftarrow t(x)=p(ax)$
for $|a|=1$ does not  fix it but  shifts $p(x)\leftarrow t(x)=p(x-c)$
for $c\neq 0$ can fix it, thus {\em enhancing the power of  estimates (\ref{eqrtrdbndsrev1}) and (\ref{eqratio0}).} 

\subsection{Classical estimates for extremal root radii}\label{subsec:class_est}

Next we recall some non-costly estimates known for the extremal root radii $r_{1}=r_{1}(0, p)$ and $r_{d}=r_{d}(0, p)$ in terms of the coefficients of $p$ (cf. \cite{kerimov1977applied}, \cite{mignotte1999polynomials}, and \cite{yap2000fundamental}) and the two parameters
\begin{equation}\label{eq:radii_at_origin_1}
\tilde{r}_{-}:=\min _{i \geq 1}\left|\frac{p_{0}}{p_{i}}\right|^{\frac{1}{i}}, \tilde{r}_{+}:=\max _{i \geq 1}\left|\frac{p_{d-i}}{p_{d}}\right|^{\frac{1}{i}}
\end{equation}
These bounds on $r_{1}$ and $r_{d}$ hold in dual pairs since $r_{1}(0, p) r_{d}\left(0, p_{\text {rev }}\right)=1$.
% (see equation (8) for $j=1$ ).
Furthermore, we have that
\begin{equation}\label{eq:radii_at_origin_2}
\frac{1}{d} \tilde{r}_{+} \leq r_{1}<2 \tilde{r}_{+}, \frac{1}{2} \tilde{r}_{-} \leq r_{d} \leq d \tilde{r}_{-},
\end{equation}
\begin{equation}\label{eq:radii_at_origin_3}
\tilde{r}_{+} \sqrt{\frac{2}{d}} \leq r_{1} \leq \frac{1+\sqrt{5}}{2} \tilde{r}_{+}<1.62 \tilde{r}_{+} \text { if } p_{d-1}=0,
\end{equation}
\begin{equation}\label{eq:radii_at_origin_4}
0.618 \tilde{r}_{-}<\frac{2}{1+\sqrt{5}} \tilde{r}_{-} \leq r_{d} \leq \sqrt{\frac{d}{2}} \tilde{r}_{-} \text { if } p_{1}=0,
\end{equation}
\begin{equation}\label{eq:radii_at_origin_5}
r_{1} \leq 1+\sum_{i=0}^{d-1}\left|\frac{p_{i}}{p_{d}}\right|, \frac{1}{r_{d}} \leq 1+\sum_{i=1}^{d}\left|\frac{p_{i}}{p_{0}}\right| .
\end{equation}
$M(p):=\left|p_{d}\right| \max _{j=1}^{d}\left\{1,\left|x_{j}\right|\right\}$ is said to be the Mahler measure of $p$, and so $M\left(p_{\text {rev }}\right):=\left|p_{0}\right| \max _{j=1}^{d}\left\{1, \frac{1}{\left|x_{j}\right|}\right\} .$ It holds that
\begin{equation}\label{eq:radii_at_origin_6}
r_{1}^{2} \leq \frac{M(p)^{2}}{\left|p_{d}\right|} \leq \max _{i=0}^{d-1}\left|\frac{p_{i}}{p_{d}}\right|^{2}, \frac{1}{r_{d}^{2}} \leq \frac{M\left(p_{\mathrm{rev}}\right)^{2}}{\left|p_{0}\right|^{2}} \leq \max _{i=1}^{d}\left|\frac{p_{i}}{p_{0}}\right|^{2}
\end{equation}
%Thm 167 - rel error bound 4d
It is shown in \cite{imbach2021rootradii} that we can get a very fast approximation of all root radii of $p$ at the origin at a very low cost, which complements the estimates \ref{eq:radii_at_origin_1}, \ref{eq:radii_at_origin_2} \ref{eq:radii_at_origin_3}, \ref{eq:radii_at_origin_4}, \ref{eq:radii_at_origin_5}, and \ref{eq:radii_at_origin_6}.

% Part 3.2 Background

One can extend all these bounds to the estimates for the root radii $r_{j}(c, p)$ for any fixed complex $c$ and all $j$ by observing that $r_{j}(c, p)=r_{j}(0, t)$ for the polynomial $t(x)=p(x-c)$ and applying Taylor's shift; \emph{i.e.,} applying the mapping $\mathcal{S}_{c ,\rho }:p(x) \mapsto p\left( \frac{x - c}{\rho} \right)$.

The algorithms in Sec.~\ref{sec:alg_des} closely approximate root radii $r_{j}(c, p)$ for a black box polynomial $p$ and a complex point $c$ at reasonably low cost, but the next well-known upper bounds on $r_{d}$ and lower bounds on $r_{1}$ (cf. \cite{kerimov1977applied},\cite{carstensen1991inclusion},\cite{pan2000approximating},\cite{bini2000design}, and \cite{bini2014solving}) are computed at even a lower cost, defined by a single fraction $\frac{p_{0}}{p_{i}}$ or $\frac{p_{d-i}}{p_{d}}$ for any $i$, albeit these bounds are excessively large for the worst case input.
Finally, we have that
$r_{d} \leq \rho_{i,-}:=\left(\left(\begin{array}{c}d \\ i\end{array}\right)\left|\frac{p_{0}}{p_{i}}\right|\right)^{\frac{1}{i}}, \frac{1}{r_{1}} \leq \frac{1}{\rho_{i,+}}:=\left(\left(\begin{array}{l}d \\ i\end{array}\right)\left|\frac{p_{d}}{p_{d-i}}\right|\right)^{\frac{1}{i}}$
and therefore, since $p^{(i)}(0)=i ! p_{i} \text { for all } i>0$, that
\begin{equation}\label{eq:last_appen}
r_{d} \leq \rho_{i,-}=\left(i !\left(\begin{array}{c}
d \\
i
\end{array}\right)\left|\frac{p(0)}{p^{(i)}(0)}\right|\right)^{\frac{1}{i}}, \frac{1}{r_{1}} \leq \frac{1}{\rho_{i,+}}=\left(i !\left(\begin{array}{c}
d \\
i
\end{array}\right)\left|\frac{p_{\text {rev }}(0)}{p_{\text {rev }}^{(i)}(0)}\right|\right)^{\frac{1}{i}}
\end{equation}
for all $i$; from which we obtain relations \ref{eqrtrdbndsrev1} for $i=1$.




\subsection{Higher order derivatives of $\frac{p'(0)}{p(0)}$}

Recall that the ratios $\frac{p'(0)}{p(0)}$ and $\frac{xp'(0)}{p(0)}$ play a key role in the root-finders and root-radii computations.
We evaluate the ratio
$p'(x)/p(x)=p_0'(x)/p_0(x)$
 by applying
the recurrence
 \begin{equation}\label{eqdndrt} \frac{p_{i+1}'(x)}{p_{i+1}(x)}=\frac{1}{2\sqrt x}\Bigg(\frac{p_{i}'(\sqrt x~)}{p_{i}(\sqrt x~)}-\frac{p_{i}'(-\sqrt x~)}{p_{i}(-\sqrt x~)}\Bigg),~i=0,1,\dots
\end{equation}
Recurrences (\ref{eqdnd}) and (\ref{eqdndrt}) reduce the evaluation  of  $p_{\ell}(c)$ to the evaluation of $p(c)$ at  $q=2^{\ell}$  points
$c^{1/q}$ and  for $c\neq 0$ reduce the evaluation of the ratio
$p_{\ell}'(x)/p_{\ell}(x)$ at $x=c$ to the evaluation
of the ratio
$p'(x)/p(x)$ at the latter $q=2^{\ell}$ points $x=c^{1/q}$.
We will see that we can apply  recurrence (\ref{eqdndrt})
to support fast convergence to the convex hull of the roots.

% Clean
Equations (\ref{eqdndrt0}) and (\ref{eqdndrth})
enable us to  strengthen
upper estimates in Eq.~\ref{eqrtrdbndsrev1}
and more generally Eq.~\ref{eq:last_appen} for root radii $r_j(0,p)$   at the origin  because $r_j(0,p_{\ell})=r_j(0,p)^{2^{\ell}}$ for $j=1,\dots,d$ (see  Eq.~\ref{eqrtrdbndsrev1}); we can  approximate the higher order derivatives
$\Big(\frac{p^{(g)}(x)}{p^{(g-1)}(x)}\Big)^{(\ell)}$
 at $x=0$ by following Remark.~\ref{rem:fin_diff}.
Besides the listed applications of
root-squaring, one can apply DLG iterations to randomized exclusion tests for sparse polynomials.
One can apply root-squaring $p(x)\mapsto p_{\ell}(x)$ to improve  the error bound for the approximation of the power sums of the roots of  $p(x)$ in the unit disc $D(0,1)$ by Cauchy sums, but the improvement is about as much as the additional cost incurred by increasing the number $q$ of points of evaluation of the ratio $\frac{p'}{p}$.
\begin{remark}\label{rescdnd}
 One can approximate the leading coefficient $p_d$ of a black box polynomial $p(x)$. This coefficient is not involved in recurrence (\ref{eqdndrt}),
 and one can apply  recurrence (\ref{eqdnd}) by using a crude
 approximation to  $p_d$
 and if needed can scale polynomials
 $p_i(x)$ for some $i$.
\end{remark}



%One common thread in the methods described above is that these hinge on higher order derivatives $\left(\frac{p'(x)}{p(x)}\right)^{(\ell)}$ and $\left(\frac{xp'(x)}{p(x)}\right)^{(\ell)}$
We can treat $\frac{p'(x)}{p(x)}$ as another blackbox that runs the same order of cost since the same blackbox oracle that evaluates $p(x)$ can be used to evaluate $p'(x)$ as well, based on the following theorem:

\begin{theorem}
Given an algorithm that evaluates a black box polynomial $p(x)$ at a point $x$ over a field $\mathcal{K}$ of constants by using $A$ additions and subtractions, $S$ scalar multiplications (that is, multiplications by elements of the field $\mathcal{K}$ ), and $M$ other multiplications and divisions, one can extend this algorithm to the evaluation at $x$ of both $p(x)$ and $p^{\prime}(x)$ by using $2 A+M$ additions and subtractions, $2 S$ scalar multiplications, and $3 M$ other multiplications and divisions.
\end{theorem}

\begin{proof}
\cite{Linnainmaa1976} and \cite{baur1983complexity} prove the theorem for any function $f\left(x_{1}, \ldots, x_{s}\right)$ that has partial derivatives in all its $s$ variables $x_{1}, \ldots, x_{s}$.
\end{proof}






\section{Our Algorithm for Evaluating $\left(\frac{p'(x)}{p(x)}\right)^{(\ell)}$}\label{sec:alg_des}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{rational_root_tree.png}
  \caption{The upper tree depicts the steps of \textsc{\textsc{Circle\_Roots\_Rational\_Form}}($p,q,l$) in Alg.\ref{alg:circ_roots_rational_form} for $l=2$, $p=1$, and $q=1$. The lower tree depicts the steps of \textsc{Roots}($r,t,u,l$) in Alg.\ref{alg:roots} for $r=2$, $l=2$, $p=1$, and $q=1$}\label{fig:rat_roots_tree}
  \caption{}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{p_prime.png}
  \caption{The steps of \textsc{DLG\_Rational\_Form}($p,p^\prime,r,t,u,l$) in Alg.\ref{alg:DLG_rational_form} for $r=2$, $l=2$, $t=1$, and $u=1$.}\label{fig:DLG}
  \caption{}
\end{figure}

In this section we present the design of the general algorithm for approximating the root radius given by Eq.~\ref{eqdndrt0}.
There are two main steps: 1) going down the rational root tree, \emph{i.e.,} performing Alg.~\ref{alg:circ_roots_rational_form}, and 2) going up the rational root tree, \emph{i.e.,} performing Alg.~\ref{alg:DLG_rational_form}. 
The going-down is depicted in Fig.~\ref{fig:rat_roots_tree} and the going-up is depicted in Fig.~\ref{fig:DLG}.
The other procedures are simple bookkeeping/preprocessing steps in between the two main going-down/going-up steps.
In particular we
1) first convert a complex number $x$ into polar coordinates $r,\theta$ where $\theta \approx \frac{p}{q} = \frac{p}{2^\epsilon}$, \emph{i.e.,} perform Alg.~\ref{alg:rational_angle_approx},
2) perform the first going-down pass which gives the rational angles for the roots of $x$ in fraction form, \emph{i.e.,} perform Alg.~\ref{alg:circ_roots_rational_form},
3) perform the second pass of the going-down algorithm where we compute the values $|x|^{\frac{1}{2^m}} \exp(2 \pi i \frac{p}{q})$ at the $m^\mathrm{th}$ level, and
4) finally compute the values given by Eq.~\ref{eqdndrt} going back up the rational root tree.

\begin{algorithm}
   \caption{\textsc{Circle\_Roots\_Rational\_Form}($p,q,l$)}
   \label{alg:circ_roots_rational_form}
\begin{algorithmic}
\IF{ $p\%q$ == 0}
  \STATE  $r, s$ := (1,1)
\ELSE
  \STATE  $r, s$ := ($p$,$2q$)
\ENDIF
  \IF{ r\%s == 0}
    \STATE $t, u$ := (1,2)
  \ELSE
    \STATE $t, u$ := $(2r+s, 2s)$
  \ENDIF
	  \IF{$l$ == 1}
		  \RETURN [($r,s$),($t,u$)]
	\ELSIF {$l$ != 0}
		\STATE left  := \textsc{Circle\_Roots\_Rational\_Form}($r,s,l-1$)
		\STATE right := \textsc{Circle\_Roots\_Rational\_Form}($t,u,l-1$)
		\RETURN left $\cup$ right
	\ELSE
		\RETURN  [($p,q$)]
      \ENDIF
\end{algorithmic}
\end{algorithm}

The intuition behind Alg.~\ref{alg:circ_roots_rational_form} is that the square root operation satisfies
\begin{equation}\label{eq:rat_sq_imag}
p \% q \neq  0 \implies   \sqrt{\exp \left(2 \pi i \frac{p}{q}\right)}  = \exp \left(2 \pi i \frac{p}{2q}\right)
\end{equation}
and
\begin{equation}\label{eq:rat_sq_real}
p \% q = 0 \implies   \sqrt{\exp \left(2 \pi i \frac{1}{1}\right)}  = \exp \left(2 \pi i \frac{1}{1}\right) = 1,
\end{equation}
and the negation operation satisfies
\begin{equation}\label{eq:rat_neg_imag}
p \% q \neq  0 \implies   -\exp \left(2 \pi i \frac{r}{s}\right)  = \exp \left(2 \pi i \frac{2r+s} {2s}\right)
\end{equation}
and
\begin{equation}\label{eq:rat_neg_real}
p \% q = 0 \implies   \sqrt{\exp \left(2 \pi i \frac{1}{1}\right)}  = \exp \left(2 \pi i \frac{1}{2}\right) = -1;
\end{equation}
therefore, the first four lines of Alg.~\ref{alg:circ_roots_rational_form} compute the (angle of the) positive square root, $\sqrt{x}$, of complex number on the unit circle and the next four lines Alg.~\ref{alg:circ_roots_rational_form} computes the (angle of the) negative square root, $-\sqrt{x}$. Therefore, we have:
\begin{theorem}\label{thm:rat_root_correctness}
  For a complex number $x$ with a rational angle $\frac{p}{q}$, \emph{i.e.,} $x = |x|\exp \left(2 \pi i \frac{p}{q}\right)$, Alg.~\ref{alg:circ_roots_rational_form} correctly computes the roots in Eq.~\ref{eqdndrt}. In particular, for a rational angle in fractional form, it does so with exact precision.
\end{theorem}
\begin{proof}
Equations \ref{eq:rat_sq_imag}, \ref{eq:rat_sq_real}, \ref{eq:rat_neg_imag}, and \ref{eq:rat_neg_real} give the base case and the theorem follows by a straightforward induction.
\end{proof}

Alg. \ref{alg:roots} and Alg. \ref{alg:rational_angle_approx} are both straightforward, so for the rest of this section we focus on the intuition behind Alg.~\ref{alg:DLG_rational_form}.

\begin{algorithm}
\caption{\textsc{Roots}($r,t,u,l$)}
\label{alg:roots}
\begin{algorithmic}
\STATE root\_tree = \textsc{Circle\_Roots\_Rational\_Form}($p,q,l$)
\STATE circ\_root = [$\exp\left(2\cdot\pi\cdot i \cdot \frac{r}{s}\right)$ for $r,s$ in root\_tree]
% \STATE circ\_root       = circ\_roots($t,u,l$)
\STATE roots =[$\sqrt[2^l]{r}\cdot$root for root in circ\_root]
\RETURN roots
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}
%    \caption{circ\_roots\_rational\_form($p,q,l$)}
%    \label{alg:circ_roots_rational_form}
% \begin{algorithmic}
%   \STATE $r, s$  := angle\_sq\_root($p,q$)
% 	\STATE $t, u$  := angle\_neg($r,s$)
% 	  \IF{$l$ == 1}
% 		  \RETURN [($r,s$),($t,u$)]
% 	\ELSIF {$l$ != 0}
% 		\STATE left  := circ\_roots\_rational\_form($r,s,l-1$)
% 		\STATE right := circ\_roots\_rational\_form($t,u,l-1$)
% 		\RETURN left $\cup$ right
% 	\ELSE
% 		\RETURN  [($p,q$)]
%       \ENDIF
% \end{algorithmic}
% \end{algorithm}


% \begin{algorithm}
% \caption{angle\_sq\_root(p,q)}
% \label{alg:angle_sq_root}
% \begin{algorithmic}
% 	\IF{ $p\%q$ == 0}
% 		\RETURN  (1,1)
% 	\ELSE
% 		\RETURN  ($p$,$2q$)
%   \ENDIF
% \end{algorithmic}
% \end{algorithm}
%
%
% \begin{algorithm}
% \caption{angle\_neg(p,q)}
% \label{alg:angle_neg}
% \begin{algorithmic}
% 	\IF{ p\%q == 0}
% 		\RETURN  (1,2)
% 	\ELSE
% 		\RETURN  $(2p+q, 2q)$
%   \ENDIF
% \end{algorithmic}
% \end{algorithm}




% \begin{algorithm}
% \caption{circ\_roots($p,q,l$):}
% \label{alg:circ_roots}
% \begin{algorithmic}
% \STATE roots = circ\_roots\_rational\_form($p,q,l$)
% \RETURN [$\exp\left(2\cdot\pi\cdot i \cdot \frac{r}{s}\right)$ for r,s in roots]
% \end{algorithmic}
% \end{algorithm}





% \clearpage

\begin{algorithm}
\caption{\textsc{DLG\_Rational\_Form}($p,p^\prime,r,t,u,l$)}
\label{alg:DLG_rational_form}
\begin{algorithmic}
\STATE 	root      := \textsc{Roots}($r,t,u,l$)
\FOR {$r_i \in $ root}
\STATE 	base\_step[$i$] := $\frac{p^\prime(r_i)}{p(r_i)}$
\ENDFOR
\STATE  diff[0]   := base\_step
\FOR {$i \leq l$}
\FOR {$j \leq 2^{l-i-1}$}
\STATE 			diff[$i+1$][$j$]:=$\frac{1}{2}\frac{\text{diff}[i][2j]-\text{diff}[i][2j+1]}{\text{root}[2j]}$
\STATE 		root = roots($r,t,u,l-1-i$)
\ENDFOR
\ENDFOR
\RETURN diff$[l][0]$
\end{algorithmic}
\end{algorithm}
Alg. \ref{alg:DLG_rational_form} is a dynamic programming on Equation \ref{eqdndrt}. Therefore, by Thm.~\ref{thm:rat_root_correctness}, we have that Alg.~\ref{alg:DLG_rational_form} correctly computes Eq.~\ref{eqdndrt}.
Specifically, Alg.~\ref{alg:DLG_rational_form} does the following: 1) it computes the last layer of the recursion Eq.~\ref{eqdndrt} (\emph{i.e.,} it computes $p'(x^{\frac{1}{2^l}})/p(x^{\frac{1}{2^l}})$) and then 2) it recursively applies Eq.~\ref{eqdndrt} via dynamic programming until it finally computes $\frac{p_{\ell}'(x)}{p_{\ell}(x)}$ which is the desired quantity.
The former is given by the line ``base\_step[$i$] := $\frac{p^\prime(r_i)}{p(r_i)}$'' and the latter is given by the line ``diff[$i+1$][$j$]:=$\frac{1}{2}\frac{\text{diff}[i][2j]-\text{diff}[i][2j+1]}{\text{root}[2j]}$'' in Alg.~\ref{alg:DLG_rational_form}; the desired output (\emph{i.e.,} $p'(x^{\frac{1}{2^l}})/p(x^{\frac{1}{2^l}})$) is given by the line ``diff$[l][0]$''.




\begin{algorithm}
\caption{\textsc{DLG}($p,p^\prime,l,x, \epsilon$)}
\label{alg:rational_angle_approx}
\begin{algorithmic}
\STATE angle     := $\frac{1}{2\pi i} \log (x)$
\STATE $u $    := $2^{\epsilon}$
\STATE$t$      :=  $(\text{angle} \cdot u)\% 1$
\STATE $r$      := $|x|$
\RETURN \textsc{DLG\_Rational\_Form}($p,p^\prime,r,t,u,l$)
\end{algorithmic}
\end{algorithm}

%
% u     = pod(2,epsilon)
% t     = mp.fmod(angle,pod(2,-epsilon))
% r     = mpf(mp.fabs(x))




% def DLG_rational_form(p,dp,r,t,u,l):
% 	root       = roots(r,t,u,l)
% 	base_step  = [dp(r)*np.reciprocal(p(r)) for r in root]
% 	derivs     = [base_step]
% 	for i in range(l):
% 		derivs.append([])
% 		for j in range(2**(l-i-1)):
% 			derivs[i+1].append((np.reciprocal(root[2*j])/2)*(derivs[i][2*j] - derivs[i][2*j+1]))
% 		root = roots(r,t,u,l-1-i)
% 	return derivs[l][0]

The final step in the algorithm is to use Alg. \ref{alg:rational_angle_approx} to compute root radius approximations $r_d$ and $r_1$.
The procedure is given by Alg.~\ref{alg:DLG_Root_Radius}.
\begin{algorithm}
\caption{\textsc{DLG\_Root\_Radius}($p,p^\prime,p_\mathrm{rev},p^\prime_\mathrm{rev},l, \epsilon, \delta $)}
\label{alg:DLG_Root_Radius}
\begin{algorithmic}
\STATE Uniformly Randomly Generate $x $ in the unit circle
\STATE $d$ := deg($p$)
\STATE $r_\mathrm{min} $ := $d/$\textsc{DLG}($p,p^\prime ,l,x\cdot 2^{-\delta}, \epsilon $)
\STATE $r_\mathrm{max} $ := \textsc{DLG}($p_\mathrm{rev},p^\prime _\mathrm{rev},l,x\cdot 2^{-\delta}, \epsilon$)$/d$
\end{algorithmic}
\end{algorithm}
The rationale for generating a random $x$ is that there may be roots close to 0 and thus, by taking the limit in certain directions, we avoid these possible poles; in particular, we have that
\begin{equation}\label{eq:lim_DLG}
  \lim_{ \delta,\epsilon \to \infty }  \textsc{DLG}(p,p^\prime ,l,x\cdot 2^{-\delta}, \epsilon ) = \frac{p_{{\ell}}'(0)}{p_{\ell}(0)}=\Big(\frac{p_{\ell-1}'(x)}{p_{\ell-1}(x)}\Big)_{x=0}'=\Big(\frac{p'(x)}{p(x)}\Big)_{x=0}^{(\ell)},
\end{equation}
for any $x$, if $p(0) \neq 0$ (See Thm~\ref{thm:lim_correctness}).





\section{Theoretical Analysis}
\label{sec:the_ana}
% \newpage
% \clearpage
% \begin{lemma}
% The (relative) condition number operator satisifies the following properties:
% \begin{enumerate}
%   \item $\kappa\{f\} (x) = |x \log'(f(x)) |$
%   \item $\kappa\{f-g\}(x) = |x \frac{ \kappa \{f\}(x)- \kappa \{g\}(x) }{f(x)-g(x)}| $
%   \item $\kappa \left\{\frac{f}{g}\right\} (x)= ||\kappa\{ f\} (x)| - |\kappa\{ g\} (x)||$
%   \item $\kappa \left\{ f \circ g \right\}(x) = \left| |\kappa\{f\} (g(x) )\cdot \kappa\{g\} (x)|\right| $
% \end{enumerate}
% \end{lemma}
% \begin{theorem}
%   The condition number for $\frac{p_l'}{p_l}$ using
% \end{theorem}
We now give some theoretical guarantees: Thm. \ref{thm:lim_correctness} proves the correctness of Alg.~\ref{alg:DLG_Root_Radius}, and Thm. \ref{thm:float_ops} and Thm. \ref{thm:ints} give computational complexity bounds.

\subsection{Correctness of the output}
\begin{theorem}\label{thm:lim_correctness}
 If $p(0) \neq 0$, then Alg. \ref{alg:DLG_Root_Radius} computes the bounds given by Eq.~\ref{eqrtrdbndsrev1}  with probability 1.
\end{theorem}
\begin{proof}
By Lem.~\ref{lem:lim_correctness} we have that the limit in Eq.~\ref{lem:lim_correctness} is well defined. Since there are at most finitely many roots for $p_\ell$ with a high probability Alg.~\ref{alg:DLG_Root_Radius} computes the correct approximation to the bounds in Eq.~\ref{eqrtrdbndsrev1} and Eq.~\ref{eq:rat_roo_radii_2}.
\end{proof}

\subsection{Complexity}
\begin{theorem}\label{thm:float_ops}
Alg. \ref{alg:DLG_rational_form} performs $q $ floating point subtractions, divisions, and multiplications and $q $ applications of $\sin$ and $\cos$, where $q = 2^l$; furthermore, Alg. \ref{alg:DLG_rational_form} performs at most $Cq $  integer additions, ``multiplications-by-2'', and $ \%2^\epsilon $ (\emph{i.e.,} mod $ 2^\epsilon $) operations, where $C=1,3,2$ respectively.
\end{theorem}
\begin{proof}
Looking at Fig.~\ref{fig:rat_roots_tree} and Fig.~\ref{fig:DLG}, we can see that the computational tree for the Alg.~\ref{alg:DLG_rational_form} is a binary tree with $2^l = q$ nodes; the proof for the constants $C=1,3,2$ follows similarly follows from the inspection of the operations performed in Alg.~\ref{alg:circ_roots_rational_form}.
\end{proof}

\begin{theorem}\label{thm:ints}
The $Cq $  integer additions, ``multiplications-by-2'', and $ \%2^\epsilon $ (\emph{i.e.,} mod $ 2^\epsilon $) operations in Alg.~\ref{alg:DLG_rational_form} have negligible overhead. More precisely, integer additions are always additions of 2 $\epsilon \log \ell $-bit integers and ``multiplications-by-2'' and $ \%2^\epsilon $ (\emph{i.e.,} mod $ 2^\epsilon $) operations have constant time overhead.
\end{theorem}

\begin{proof}
  Since Alg.~\ref{alg:rational_angle_approx} always passes in a denominator which is a power of two all of the integer $\%$ and $\cdot$ operations are in fact ``multiplications-by-2'', and $ \%2^\epsilon $ (\emph{i.e.,} mod $ 2^\epsilon $) operations by a straightforward proof similar to the one in Alg.~\ref{alg:DLG_rational_form}. Thus these operation are essentially constant overhead bit shift operations on a computing machine with binary words. Since $p\% r$ always reduces $p \mapsto 1$, we have that whenever an overflow of more than $\log r$-bits happens in Alg.~\ref{alg:circ_roots_rational_form} it gets converted to an $\log r$-bit integer; therefore, it suffices to prove that $\log r$ is bounded by  $\epsilon \log \ell $. However, this once again follows by induction on the binary computation tree: since this tree has depth $\ell$ we see that any denominator is bounded by $\epsilon \log \ell $ by a simple induction.
\end{proof}

In order to prove Thm.~\ref{thm:lim_correctness} we must first prove Lem.~\ref{lem:lim_correctness}.
\begin{lemma}\label{lem:lim_correctness}
   If $p(0) \neq 0$, then the limit $ \lim_{x \to 0}\frac{p_\ell^{\prime}(x)}{p_\ell(x)}$ is always well-defined.
   \end{lemma}
\begin{proof}
By applying induction on Eq. \ref{eqdnd}, we have that $p_l(0) \neq 0$ if $p(0) \neq 0$, and thus it suffices to consider the behavior of the numerator in  Eq.~\ref{eqdndrt}. The case $\ell = 1$ follows from an application of L'Hopital's rule. For $\ell \neq 1$, there are two cases: either $\sqrt x~$ divides the numerator of $\frac{p_{\ell}'(\sqrt x~)}{p_{\ell}(\sqrt x~)}$ or it does not. If it does, then we are done since we can once again apply L'Hopital's rule. Otherwise, we get that $p_{\ell}'(\sqrt x~) = c_0+c_1\sqrt x~ + ... +c_k(\sqrt x~)^k$ for some $c_i$ with $c_0 \neq 0$. But then we have
\begin{align*}
\begin{split}
\frac{p_{\ell+1}'(\sqrt x~)}{p_{\ell+1}(\sqrt x~)}
&= \frac{1}{2\sqrt x}\Big(\frac{p_{\ell}'(\sqrt x~)}{p_{\ell}(\sqrt x~)}-\frac{p_{\ell}'(-\sqrt x~)}{p_{\ell}(-\sqrt x~)}\Big) \\
%& = \frac{1}{2\sqrt x}\frac{b_0c_0 + \sqrt x \cdot N(\sqrt x) - b_0c_0 - \sqrt x \cdot N'(\sqrt x)}{p_{\ell}'(\sqrt x~)p_{\ell}'(-\sqrt x~)}\\
& = \frac{1}{2\sqrt x}\frac{b_0c_0 + \sqrt x \cdot N_1(\sqrt x) - b_0c_0 - \sqrt x \cdot N_2(\sqrt x)}{p_{\ell}'(\sqrt x~)p_{\ell}'(-\sqrt x~)}\\
& = \frac{1}{2}\frac{  N_1(\sqrt x) -  N_2(\sqrt x)}{p_{\ell}'(\sqrt x~)p_{\ell}'(-\sqrt x~)},
\end{split}
\end{align*}
 for some polynomials $N_1$ and $N_2$ with $b_0 = p_{\ell}'(0)$. Therefore the limit at zero is once again well-defined.
\end{proof}
\subsection{Stability}

\begin{lemma}
The (relative) condition number operator $\kappa$ satisfies the following properties:
\begin{enumerate}
  \item $\kappa\{f\} (x) = |x \log'(f(x)) |$
  % \item $\kappa\{f-g\}(x) = |x \frac{ \kappa \{f\}(x)- \kappa \{g\}(x) }{f(x)-g(x)}| $
  \item $\kappa \left\{\frac{f}{g}\right\} (x)= ||\kappa\{ f\} (x)| - |\kappa\{ g\} (x)||$
    \item $\kappa \{x^ d \} (x)= d$
  % \item $\kappa \left\{ f \circ g \right\}(x) = \left| |\kappa\{f\} (g(x) )\cdot \kappa\{g\} (x)|\right| $
\end{enumerate}
\end{lemma}
\begin{proof}
This follows from the definition of the condition number.
\end{proof}
\begin{theorem}\label{thm:stability}
If $p_\ell(0)\neq 0$, then $\frac{p_{\ell}'(x)}{p_{\ell}(x)}$ is well-conditioned at any point sufficiently close to 0.
\end{theorem}
\begin{proof}
  The proof of Lem.~\ref{lem:lim_correctness} gives us that $\frac{p_{\ell}'(x)}{p_{\ell}(x)}$ is a well behaved rational function at any point close to zero and thus the condition number $\kappa\{\frac{p_{\ell}'}{p_{\ell}}\}(x)$ is well defined at this point since the condition number of arithmetic operations of functions are themselves arithmetic operations in those same functions and the conditions number for a polynomial of degree $d \in \mathbb{R}$ is exactly $d$; therefore, $\kappa\{\frac{p_{\ell}'}{p_{\ell}}\}(x)$ has a well defined/bounded condition number in the limit to zero.
\end{proof}

\begin{remark}
Even though Thm.~\ref{thm:stability} states that $\frac{p_{\ell}'(x)}{p_{\ell}(x)}$ is highly stable, in practice, computating this function requires the use of trigonometric functions (\emph{i.e.,} the roots in Alg.~\ref{alg:roots}); therefore, our added precision in Alg..~\ref{alg:circ_roots_rational_form} helps with the instability associated with Alg.~\ref{alg:roots}. Intuitively this helps the instability because the trigonometric functions are the only subroutines in the algorithm that have a non constant condition number and thus, special care must be taken with them.
\end{remark}


\section{Experimental Results}
\subsection{Setup}
\subsection{Tables}
\subsection{Observations}
\section{Conclusion}


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{splncs04}
\bibliography{ref.bib}

%%
%% If your work has an appendix, this is the place to put it.
\appendix





\end{document}



\section{Approximation of all roots}\label{sall}
The latter algorithms only approximate the absolutely smallest 
root. By applying them  to the reverse polynomial we can approximate the absolutely largest 
root. For the approximation of the roots $x_j$, say, for $j$ near $d/2$ where $d$ is  large, however, all these
 algorithms involve some coefficients 
 $p_i^{(k)}$ of $p_k(x)$ for 
 $i$ near $d/2$ and large $2^k$.
 Consequently the computations are prone to severe  problems of  numerical stability.

Recursive deflation can be a way out. Having approximated the absolutely
smallest root $x_d$ of  $f_0(x):=p(x)$, we can apply the same  algorithm to the  polynomial 
$f_0(x):=p(x)/(x-x_d)$ of degree $d-1$  and then repeat this recipe recursively, that is,  approximate
the absolutely
smallest roots of  polynomials
$f_j(x)$ of degrees $d-j$, for 
$j=1,\dots,d-1$, finally approximating all the $d$ roots. 


It is well known that recursive deflation destroys sparsity of $p(x)$ and generally blows up the overall  coefficient length (e.g., let $p(x)=x^d+1$), but  we avoid these problems by applying deflation as well as  DLG or FG root-finders to  the polynomials $f_j(x)$ implicitly, handling them as black box polynomials without
computing their coefficients.
This increases the overall number
 of black box evaluation points by a factor of $d$ and add divisions by the values $\prod_{j=0}^k(x-x_{d-j})$
 for $k=0,1,\dots,d-1$ for selected
 points $x$. If the complexity 
  of the evaluation of 
 $p(x)$ dominates that of such a product, then implicit deflation increases the overall complexity 
 of root-finding  by at most a factor of $3d$.	 

\section{Estimation of extremal root radii}\label{sestextrrtrd}


If we do not know whether assumptions (\ref{eqnndcrs}) and (\ref{eqsprn}) hold, we cannot apply  Eq. (\ref{eqrtrt}) and
Thm. \ref{thgmn}, but we  can apply
 the following
well-known bounds on the extremal root radii:

 \begin{equation}\label{eqrtrdbndsrev1} 
|x_d|\le d~\Big |\frac{p(0)}{p'(0)}\Big |~{\rm and}~|x_1|\ge \Big |\frac{p'_{\rm rev}(0)}{d~p_{\rm rev}(0)}\Big|.
\end{equation}

We can deduce these bounds 
from the well-known expression
\begin{equation}\label{eqratio}
\frac{p'(x)}{p(x)}=
\sum_{j=1}^d\frac{1}{x-x_j},
\end{equation} 
which we can obtain by differentiating the equation
$p(x)=p_d\prod_{j=1}^d(x-x_j)$.

By extending these bounds to
the polynomial $p_k(x)$ of Eq. (\ref{eqdnd}) we obtain that

 \begin{equation}\label{eqratio0} 
 |x_d|^{2^k}\le d/ 
\Big| \Big(\frac{p'(x)}{p(x)}\Big)_{x=0}^{(k)}\Big|~{\rm and}~|x_1|^{2^k}\ge \frac{1}{d}\Big| \Big(\frac{p'_{\rm rev}(x)}{p_{\rm rev}(x)}\Big)_{x=0}^{(k)}\Big|.
\end{equation}

Under the 
assumptions (\ref{eqnndcrs}) for $i=2$ for $p(x)$ and for $p_{\rm rev}(x)$, respectively,  the latter two bounds 
 become sharp as $k$ increases, by virtue of (\ref{eqrtrdbndsrev1}), and next we  argue informally that it tends to be sharp with a high probability under  random root models.
Indeed, 
\begin{equation}\label{eqratiorcp}
\frac{1}{|x_d|}\le \frac{1}{d}\Big|\frac{p'(c)}{p(c)}\Big|=\frac{1}{d}\Big|\sum_{j=1}^d \frac{1}{c-x_j}\Big|
\end{equation}  
by  virtue of 
(\ref{eqrtrdbndsrev1}), and so the
 approximation  
to the root radius $|x_d|$ is poor if and only if severe cancellation occurs in the summation of the $d$ roots, and similarly for the approximation
of  $r_1(c,p)$. Such a cancellation only occurs for a narrow class of polynomials $p(x)$, with a low  probability if we assume a random root model.
 
 Next we prove, however,  that estimates (\ref{eqrtrdbndsrev1}) and (\ref{eqratio0}) are extremely poor for  worst case inputs.
\begin{theorem}\label{thrtr}
The ratios $|\frac{p(0)}{p'(0)}|$ 
and $|\frac{p_{\rm rev}(0)}{p_{\rm rev}'(0)}|$ are infinite for  $p(x)=x^d-h^d$ and $h\neq 0$, while  $|x_d|)=|x_1|=|h|$.
 \end{theorem}
 \begin{proof}
% \begin{example}\label{exrtr} 
 Observe that 
 the roots $x_j=h\exp(\frac{(j-1){\bf i}}{2\pi d})$ of $p(x)=x^d-h^d$
  for $j=1,2,\dots,d$   are the $d$th roots  of unity up to scaling by $h$.
 \end{proof}
%Such poor approximations only occur for a %narrow input class.  

Clearly, the problem persists for the root radius $r_d(w,p)$  where  $p'(w)$ and  $p'_{\rm rev}(w)$ vanish;  
 rotation of the variable  $p(x)\leftarrow t(x)=p(ax)$
for $|a|=1$ does not  fix it but  shifts $p(x)\leftarrow t(x)=p(x-c)$
for $c\neq 0$ can fix it, thus {\em enhancing the power of  estimates (\ref{eqrtrdbndsrev1}) and (\ref{eqratio0}).} 

